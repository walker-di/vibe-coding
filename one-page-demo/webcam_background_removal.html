<!DOCTYPE html>
<html>
<head>
    <title>Configurable Background Removal with Fabric.js</title>
    <style>
        body {
            font-family: sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            margin-top: 20px;
            background-color: #f0f0f0; /* Light gray background to see transparency */
        }
        h1 {
            margin-bottom: 10px;
        }
        #config {
            margin-bottom: 15px;
            padding: 10px;
            border: 1px solid #eee;
            border-radius: 5px;
            background-color: #f9f9f9;
            display: flex; /* Arrange items horizontally */
            gap: 20px; /* Space between config groups */
            flex-wrap: wrap; /* Wrap on smaller screens */
            justify-content: center;
        }
        #config label {
            margin-right: 5px;
            font-weight: bold;
        }
        #config .config-group {
            display: flex;
            align-items: center;
        }
         #config .config-group label {
             font-weight: normal; /* Labels inside group are not bold */
             margin-right: 10px;
         }

         /* Style the file input to look like a button */
         .file-input-container {
             display: inline-block;
             position: relative;
             overflow: hidden;
             margin-left: 10px;
         }

         .file-input-container input[type=file] {
             position: absolute;
             left: 0;
             top: 0;
             opacity: 0;
             cursor: pointer;
         }

         .file-input-button {
             display: inline-block;
             padding: 5px 10px;
             background-color: #007bff;
             color: white;
             border: none;
             border-radius: 4px;
             cursor: pointer;
             font-size: 0.9em;
         }

         .file-input-button:hover {
             background-color: #0056b3;
         }

         #backgroundFileName {
             margin-left: 10px;
             font-style: italic;
             color: #555;
             max-width: 150px; /* Limit width */
             overflow: hidden;
             text-overflow: ellipsis; /* Add ellipsis if filename is too long */
             white-space: nowrap;
         }
         #clearBackgroundButton {
             margin-left: 5px;
             padding: 2px 6px;
             font-size: 0.8em;
             background-color: #dc3545;
             color: white;
             border: none;
             border-radius: 4px;
             cursor: pointer;
             display: none; /* Hide initially */
         }
         #clearBackgroundButton:hover {
              background-color: #c82333;
         }

        #fabricControls {
             margin-top: 10px;
             padding: 10px;
             border: 1px solid #eee;
             border-radius: 5px;
             background-color: #f9f9f9;
             display: flex;
             gap: 10px;
             flex-wrap: wrap;
             justify-content: center;
             margin-bottom: 20px;
        }

        #fabricControls .file-input-container {
            margin-left: 0; /* Reset margin for Fabric controls */
        }


        #container {
            position: relative;
            width: auto; /* Adjust based on video/canvas size */
            height: auto;
            margin-bottom: 20px;
            border: 1px solid #ccc;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
            /* Minimum size to prevent collapse before video loads */
            min-width: 320px;
            min-height: 240px;
        }

        /* The canvas element will be managed by Fabric.js */
        #videoOutput {
            display: block; /* Remove extra space below element */
            max-width: 100%;
            height: auto;
        }

        #videoInput {
            display: none; /* Hide the raw video feed */
        }

        #controls {
            margin-top: 10px;
            display: flex;
            gap: 10px;
        }
        #loadingMessage {
            margin-bottom: 10px;
            font-style: italic;
            color: #555;
            min-height: 1.2em; /* Reserve space to prevent layout shift */
            text-align: center;
        }
        #downloadLink {
            display: none;
            margin-top: 10px;
            padding: 10px;
            background-color: #e9e9e9;
            border-radius: 5px;
            text-decoration: none;
            color: #333;
        }
        #downloadLink:hover {
            background-color: #dcdcdc;
        }
        button:disabled, select:disabled, .file-input-button:disabled {
            opacity: 0.6;
            cursor: not-allowed;
        }

        /* Fabric.js adds its own canvas elements for interaction layer, if using overlay.
           But we are using a single canvas strategy, so this might not be needed.
           If you switch to overlay strategy, you'd need styles like:
           .canvas-container { position: relative; width: ...; height: ...; }
           .canvas-container canvas { position: absolute; top: 0; left: 0; }
           #videoOutput { z-index: 1; }
           .upper-canvas { z-index: 2; }
        */

    </style>
</head>
<body>

    <h1>Real-time Background Removal with Graphics</h1>

    <div id="loadingMessage">Loading TensorFlow.js and model...</div>

    <div id="config">
        <div class="config-group">
            <label for="modelTypeSelect">Model Type:</label>
            <select id="modelTypeSelect">
                <option value="general" selected>General Purpose</option>
                <option value="landscape">Landscape Optimized</option>
            </select>
        </div>
        <div class="config-group">
            <label>Background Image:</label>
             <div class="file-input-container">
                 <button class="file-input-button">Choose Background</button>
                 <input type="file" id="backgroundImageInput" accept="image/*">
             </div>
             <span id="backgroundFileName">No image selected</span>
             <button id="clearBackgroundButton">X</button>
        </div>
    </div>

    <!-- New controls for adding Fabric.js elements -->
    <div id="fabricControls">
        <label>Add Elements:</label>
         <div class="file-input-container">
             <button class="file-input-button">Add Image</button>
             <input type="file" id="addImageInput" accept="image/*">
         </div>
         <button id="addTextButton">Add Text</button>
         <button id="deleteSelectedButton">Delete Selected</button>
    </div>


    <div id="container">
        <!-- Hidden video element for raw camera feed -->
        <video id="videoInput" playsinline autoplay muted></video>
        <!-- Canvas element for processed output, managed by Fabric.js -->
        <canvas id="videoOutput"></canvas>
    </div>

    <div id="controls">
        <button id="startButton" disabled>Start Camera</button>
        <button id="stopButton" disabled>Stop Camera</button>
        <button id="startRecordingButton" disabled>Start Recording</button>
        <button id="stopRecordingButton" disabled>Stop Recording</button>
    </div>

    <a id="downloadLink" style="display: none;">Download Recording</a>

    <pre>
        <h3>Todo</h3>
        It appears you've built a web application that uses TensorFlow.js and MediaPipe Selfie Segmentation to remove the background from a webcam feed and overlay it onto a Fabric.js canvas with custom backgrounds and elements. This is a common and effective approach for in-browser background removal.

You're experiencing "noise" and asking about better models or ways to improve quality. Here's a breakdown of options based on the search results and the code you provided:

**1. Alternative Models:**

*   **MediaPipe Selfie Segmentation (You are already using this):** This is a state-of-the-art model optimized for real-time performance in selfie/video conferencing scenarios where the person is relatively close to the camera (< 2m). It's actively used in Google products like Google Meet. Your code already allows switching between the `general` (256x256 input) and `landscape` (144x256 input) models. The `general` model is typically more accurate but slightly slower than `landscape`. You can try ensuring you are using the `general` model if performance allows, as it processes a higher-resolution input.
*   **BodyPix:** Mentioned as another TensorFlow.js model for body segmentation. BodyPix can segment individuals and even body parts, and works for multiple people. It might be worth exploring if it handles specific types of noise or artifacts differently, although MediaPipe Selfie Segmentation is generally considered highly optimized for selfie/video use cases.
*   **BlazePose GHUM:** This model, also available through the TensorFlow.js body-segmentation API, provides full body segmentation and 3D pose estimation simultaneously. It's designed for full-body scenarios up to 4 meters from the camera, while Selfie Segmentation is for closer ranges. If your "noise" is related to parts of the body being incorrectly segmented at a distance, BlazePose GHUM *might* be an alternative, but it's likely overkill if you only need segmentation and are close to the camera.
*   **Other In-Browser Libraries:** There are libraries like `@imgly/background-removal` that offer in-browser background removal using different underlying models (often ONNX models). These are specifically designed for background removal and might offer different quality characteristics or handle certain types of noise better. Some offer various model sizes with different trade-offs between quality and download size/performance. A Hugging Face Space also demonstrates background removal using `Transformers.js` and the RMBG V1.4 model. Implementing one of these would require integrating a different library into your application.

**2. Post-Processing Techniques:**

*   **Smoothing the Mask:** The segmentation mask itself might have noisy or jagged edges. Applying a smoothing filter to the mask *before* using it to cut out the person can help create a cleaner edge. A **bilateral filter** is mentioned as an edge-preserving blur that can smooth mask edges and reduce pixelation and noise without blurring sharp features significantly. Slack engineering used this approach with MediaPipe Selfie Segmentation to improve mask quality.
*   **Mask Thresholding:** The segmentation models often output a probability mask (values between 0 and 1) rather than a strict binary (0 or 1) mask. Your code uses `segmentation.mask.toCanvasImageSource()` which likely handles this internally, but explicitly thresholding the mask (e.g., pixels with probability > 0.6 are foreground, otherwise background) can sometimes refine the edges. Your current implementation relies on the default behavior of `toCanvasImageSource`.
*   **Edge Blurring:** Blurring the edge between the foreground and background can make transitions smoother and hide minor segmentation inaccuracies. The `body-segmentation` API documentation mentions an `edgeBlurAmount` option for drawing masks, though your code creates the masked image on a temporary canvas manually using `destination-in`. You could potentially implement a blur effect on the edges of the masked person image on the temporary canvas before transferring it to Fabric.js.
*   **Post-Processing the Cutout:** Once you have the masked person on your `tempCanvas`, you could potentially apply image processing filters (like noise reduction filters) directly to the pixel data of the `tempCanvas` before converting it to a data URL for Fabric.js. This is more complex as it requires direct pixel manipulation or using another image processing library in conjunction with canvas operations.
*   **Morphological Operations:** Techniques like morphological opening or closing (common in image processing libraries like OpenCV) can be used to clean up small artifacts or holes in binary masks. While not a direct option in your current pure JavaScript setup without a dedicated image processing library, it's a standard approach for mask refinement.

**3. Model Configuration and Input:**

*   **`modelType`:** As mentioned, you can switch between `general` and `landscape`. Ensure you've tested the `general` model if your hardware can handle the slightly higher computational cost.
*   **`model_complexity` (Not directly in your current code, but relevant for MediaPipe in general):** For some MediaPipe solutions (like Pose Detection), there are complexity options (0, 1, or 2) where higher values increase accuracy at the expense of latency. While your code uses the `MediaPipeSelfieSegmentation` model *via* the `body-segmentation` API, and this API currently primarily exposes `modelType`, more complex options *might* exist or be exposed in other ways if you were using the raw MediaPipe API directly. The `body-segmentation` API currently offers `modelType` which corresponds to selecting different MobileNetV3-based models.
*   **`foregroundThreshold`:** The `body-segmentation` API's `drawMask` function accepts a `foregroundThreshold`. While you're not using `drawMask` directly, the underlying segmentation mask has probability values. Adjusting how these probabilities are converted to a binary mask could affect the result. Your current method using `destination-in` with `toCanvasImageSource` might implicitly use a default threshold or method.
*   **Input Video Quality:** The "noise" might be present in the original webcam feed, especially in low light conditions. While less directly controllable via code (beyond requesting higher resolution via `getUserMedia`), better lighting can significantly improve segmentation quality.

**How to Improve Your Code:**

Based on the search results, a practical approach to reduce noise and improve edge quality with your current MediaPipe setup would be to implement mask post-processing:

1.  **Get the raw segmentation mask:** Instead of directly converting the mask to a `CanvasImageSource` and drawing it, get the raw pixel data of the mask. The `body-segmentation` API has functions like `toBinaryMask` which can give you a canvas or image data representing the mask, potentially allowing more control over thresholding.
2.  **Apply a smoothing filter:** Implement or use a library function for a bilateral filter or Gaussian blur on the raw mask data or a temporary canvas drawing the mask.
3.  **Refine the cutout:** Use the smoothed mask to perform the 'destination-in' compositing operation on your temporary canvas containing the raw video frame.
4.  **Transfer to Fabric.js:** Convert the resulting `tempCanvas` to a Data URL and update your `videoObject` in Fabric.js.

Alternatively, investigate the `@imgly/background-removal` library as a potential drop-in replacement for the segmentation part, as it's specifically designed for high-quality background removal.

**In summary:**

*   **Better Models:** While MediaPipe Selfie Segmentation is good, you could explore **BodyPix** (from the same TF.js body-segmentation library) or dedicated background removal libraries like **@imgly/background-removal**.
*   **Better Ways (Techniques):** Implement **post-processing** on the segmentation mask (smoothing, thresholding) to refine the edges and reduce noise artifacts before creating the final masked image. Adjusting the **`modelType`** you are using in your code to `general` might also offer better quality.

Implementing mask post-processing (like bilateral smoothing) would be the most direct way to address noise and improve edge quality using your existing MediaPipe setup, as suggested by the Slack engineering example. This would involve modifying your `processFrame` function to handle the mask data more explicitly before the `destination-in` step.
    </div>

    <!-- TensorFlow.js libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>

    <!-- MediaPipe Selfie Segmentation -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation"></script>

    <!-- Body-segmentation library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation@latest/dist/body-segmentation.min.js"></script>

    <!-- Fabric.js library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fabric.js/5.3.1/fabric.min.js"></script>


    <script>
        const videoInput = document.getElementById('videoInput');
        const videoOutput = document.getElementById('videoOutput'); // This canvas will be managed by Fabric.js
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const startRecordingButton = document.getElementById('startRecordingButton');
        const stopRecordingButton = document.getElementById('stopRecordingButton');
        const downloadLink = document.getElementById('downloadLink');
        const loadingMessage = document.getElementById('loadingMessage');
        const modelTypeSelect = document.getElementById('modelTypeSelect');
        const backgroundImageInput = document.getElementById('backgroundImageInput'); // File input for static background
        const backgroundFileNameSpan = document.getElementById('backgroundFileName'); // Span for background filename
        const clearBackgroundButton = document.getElementById('clearBackgroundButton'); // Clear background button
        const fileInputButtons = document.querySelectorAll('.file-input-button'); // All file input buttons

        // New elements for Fabric.js controls
        const addImageInput = document.getElementById('addImageInput'); // File input for adding images
        const addTextButton = document.getElementById('addTextButton');
        const deleteSelectedButton = document.getElementById('deleteSelectedButton');
        const fabricControlsDiv = document.getElementById('fabricControls');

        let stream = null;
        let segmenter = null; // Renamed from 'model' for clarity (it's the segmenter instance)
        let rafId = null; // requestAnimationFrame ID
        let mediaRecorder = null;
        let recordedChunks = [];
        let isProcessing = false;
        let isLoadingModel = false; // Flag to prevent multiple loadModel calls at once

        let fabricCanvas = null; // The Fabric.js canvas instance
        let videoObject = null; // The Fabric.js image object representing the video feed (masked person)

        // Create a temporary canvas for drawing the masked person frame *before* converting to a Fabric.js image
        let tempCanvas = document.createElement('canvas');
        let tempCtx = tempCanvas.getContext('2d', { willReadFrequently: true }); // willReadFrequently can help performance


        // --- Model Loading ---
        async function loadModel() {
            // Prevent multiple simultaneous model loads
            if (isLoadingModel) {
                console.log("Model is already loading or being reloaded. Please wait.");
                return;
            }
            isLoadingModel = true;
            stopCamera(); // Stop camera before changing/reloading the model
             // Also stop/dispose Fabric.js canvas if it exists
             disposeFabricCanvas();

            // Disable relevant controls while loading
            startButton.disabled = true;
            modelTypeSelect.disabled = true;
            backgroundImageInput.disabled = true;
            fileInputButtons.forEach(btn => btn.disabled = true); // Disable all file input buttons
            clearBackgroundButton.disabled = true;
            addTextButton.disabled = true; // Disable new fabric controls
            deleteSelectedButton.disabled = true;
            fabricControlsDiv.style.pointerEvents = 'none'; // Visually indicate disabled state for control container


            loadingMessage.innerText = 'Loading TensorFlow.js and model...'; // Reset message

            try {
                // Ensure tf is loaded before using it
                if (typeof tf === 'undefined') {
                    throw new Error("TensorFlow.js (tf) not loaded.");
                }
                // Check for WebGL backend availability and set it
                 if (!tf.backend() || tf.getBackend() !== 'webgl') {
                    console.log('Setting backend to webgl...');
                    await tf.setBackend('webgl');
                    console.log('Backend set to', tf.getBackend());
                 }

                loadingMessage.innerText = 'Loading segmentation model...';

                // Ensure bodySegmentation is loaded before using it
                 if (typeof bodySegmentation === 'undefined') {
                    throw new Error("Segmentation model library not loaded (bodySegmentation undefined). Please check CDN link or network.");
                 }
                 if (typeof bodySegmentation.createSegmenter !== 'function') {
                     throw new Error("bodySegmentation.createSegmenter function not found. Library may not be fully loaded or is incorrect version.");
                 }
                // Ensure MediaPipe SelfieSegmentation is available globally
                 if (typeof SelfieSegmentation === 'undefined') {
                     throw new Error("MediaPipe Selfie Segmentation library not loaded. Please check CDN link for @mediapipe/selfie_segmentation.");
                 }
                 // Ensure Fabric.js is available globally
                 if (typeof fabric === 'undefined') {
                     throw new Error("Fabric.js library not loaded. Please check CDN link for fabric.js.");
                 }


                // Get the selected model type from the dropdown
                const selectedModelType = modelTypeSelect.value;

                // Load the MediaPipe Selfie Segmentation model using createSegmenter
                const modelDefinition = bodySegmentation.SupportedModels.MediaPipeSelfieSegmentation;
                const segmenterConfig = {
                    runtime: 'mediapipe', // Use 'mediapipe' runtime for MediaPipe models
                    solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation',
                    modelType: selectedModelType // Use the selected value
                };
                console.log('Creating segmenter with config:', segmenterConfig);

                // Dispose of the previous segmenter model if it exists
                if (segmenter && typeof segmenter.dispose === 'function') {
                     try {
                        await segmenter.dispose();
                        console.log("Previous segmenter disposed.");
                     } catch (disposeError) {
                        console.warn("Error disposing previous segmenter:", disposeError);
                     }
                }

                // Use createSegmenter to get the segmenter instance
                segmenter = await bodySegmentation.createSegmenter(modelDefinition, segmenterConfig);
                console.log('Segmenter created:', segmenter);


                loadingMessage.innerText = `Model '${selectedModelType}' loaded. Click "Start Camera".`;
                startButton.disabled = false;

            } catch (error) {
                segmenter = null; // Ensure segmenter is null on error
                loadingMessage.innerText = `Error loading model: ${error.message}. Select a different model or check console.`;
                console.error('Error loading model:', error);
                startButton.disabled = true; // Keep start button disabled if model load failed
            } finally {
                 // Re-enable the dropdown and file input after loading attempt finishes
                 modelTypeSelect.disabled = false;
                 backgroundImageInput.disabled = false;
                 fileInputButtons.forEach(btn => btn.disabled = false); // Re-enable all file input buttons
                 // Re-enable clear button only if an image is loaded (logic handled by check in clearBackgroundImage)
                 if (backgroundImageInput.value) { // Check file input value to see if an image was loaded
                     clearBackgroundButton.disabled = false;
                 } else {
                     clearBackgroundButton.disabled = true; // Ensure it's disabled if no image
                 }

                 // Fabric controls enabled only if model loaded successfully
                 if (segmenter) {
                     addTextButton.disabled = false;
                     deleteSelectedButton.disabled = false; // Delete can be enabled even with no objects
                     fabricControlsDiv.style.pointerEvents = 'auto'; // Enable clicks on container
                 } else {
                     addTextButton.disabled = true;
                     deleteSelectedButton.disabled = true;
                     fabricControlsDiv.style.pointerEvents = 'none';
                 }


                 isLoadingModel = false;
            }
        }

        // --- Camera Access ---
        async function startCamera() {
            if (!segmenter) {
                console.warn("Model (segmenter) is not loaded yet.");
                loadingMessage.innerText = "Model not loaded. Please wait or check for errors.";
                return; // Don't start if model isn't ready
            }

            downloadLink.style.display = 'none'; // Hide previous download link
            downloadLink.href = ''; // Clear previous blob URL

            // Disable config dropdown and file input while camera is active
            modelTypeSelect.disabled = true;
            backgroundImageInput.disabled = true;
            fileInputButtons.forEach(btn => btn.disabled = true); // Disable all file input buttons
            clearBackgroundButton.disabled = true;

            // Disable fabric controls while camera is connecting/starting
            addTextButton.disabled = true;
            deleteSelectedButton.disabled = true;
            fabricControlsDiv.style.pointerEvents = 'none';


            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: 'user'
                    },
                    audio: true // Optional: include audio for recording
                });

                videoInput.srcObject = stream;

                videoInput.onloadedmetadata = () => {
                    videoInput.play();
                    videoInput.style.display = 'none'; // Ensure raw video is hidden

                    // Wait for video to be ready before starting processing
                    videoInput.addEventListener('canplay', () => {
                         // Set canvas dimensions to match video
                         videoOutput.width = videoInput.videoWidth;
                         videoOutput.height = videoInput.videoHeight;

                         // Initialize or resize Fabric.js canvas
                         if (!fabricCanvas) {
                             fabricCanvas = new fabric.Canvas('videoOutput', {
                                 selection: true, // Allow object selection
                                 backgroundColor: 'transparent' // Default background is transparent
                             });
                             console.log("Fabric.js canvas initialized.");

                             // Add event listeners for Fabric.js events if needed (e.g., selection)
                              fabricCanvas.on('selection:created', updateDeleteButtonState);
                              fabricCanvas.on('selection:updated', updateDeleteButtonState);
                              fabricCanvas.on('selection:cleared', updateDeleteButtonState);
                              fabricCanvas.on('object:added', updateDeleteButtonState);
                              fabricCanvas.on('object:removed', updateDeleteButtonState);
                              // Initial state check
                              updateDeleteButtonState();

                         } else {
                            // Resize existing Fabric canvas
                            fabricCanvas.setDimensions({
                                width: videoInput.videoWidth,
                                height: videoInput.videoHeight
                            });
                             // Fabric objects might need scaling if aspect ratio changes significantly,
                             // but we'll skip that for this example. User can reposition/scale manually.
                         }

                         // Initialize or resize temporary canvas for masked cutout
                         tempCanvas.width = videoInput.videoWidth;
                         tempCanvas.height = videoInput.videoHeight;
                         tempCtx = tempCanvas.getContext('2d', { willReadFrequently: true });


                         // Start processing and recording setup
                        isProcessing = true;
                        processFrame(); // Start the processing loop

                        startButton.disabled = true;
                        stopButton.disabled = false;
                        startRecordingButton.disabled = false;
                        stopRecordingButton.disabled = true; // Recording starts only when explicitly clicked
                        loadingMessage.style.display = 'none'; // Hide message during active use

                         // Enable fabric controls now that camera is running
                         addTextButton.disabled = false;
                         deleteSelectedButton.disabled = false; // Still enabled as objects can be added/removed
                         fabricControlsDiv.style.pointerEvents = 'auto';


                    }, { once: true }); // Only run this setup once
                };

            } catch (error) {
                loadingMessage.style.display = 'block'; // Show message again on error
                loadingMessage.innerText = `Error accessing camera: ${error.message}`;
                console.error('Error accessing camera:', error);
                startButton.disabled = false; // Allow retrying
                // Re-enable config on camera error
                modelTypeSelect.disabled = false;
                backgroundImageInput.disabled = false;
                fileInputButtons.forEach(btn => btn.disabled = false);
                if (backgroundImageInput.value) {
                    clearBackgroundButton.disabled = false;
                 }

                // Ensure fabric controls are disabled if camera fails
                 addTextButton.disabled = true;
                 deleteSelectedButton.disabled = true;
                 fabricControlsDiv.style.pointerEvents = 'none';
            }
        }

        function stopCamera() {
             if (rafId) {
                cancelAnimationFrame(rafId);
                rafId = null;
            }
             isProcessing = false; // Stop the processing loop flag

             if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                 // Stop recording if ongoing when camera is stopped
                 console.log("Stopping recording because camera is stopping.");
                 mediaRecorder.stop();
             }

            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
            }

             // Clear the canvases and Fabric.js objects
             if (fabricCanvas) {
                 fabricCanvas.clear(); // Clears all objects and background image
                 fabricCanvas.renderAll(); // Ensure canvas is visually cleared
             }
             videoObject = null; // Clear the reference to the video image object

             // Dispose temporary canvas context (not strictly necessary for 2D, but good practice)
             if (tempCtx) {
                tempCtx.clearRect(0, 0, tempCanvas.width, tempCanvas.height);
                // tempCanvas = null; // Could null the element if needed, re-create on start
                // tempCtx = null;
             }


            videoInput.srcObject = null; // Disconnect stream from video element

            startButton.disabled = false;
            stopButton.disabled = true;
            startRecordingButton.disabled = true; // Disable recording when camera stops
            stopRecordingButton.disabled = true;

            loadingMessage.style.display = 'block'; // Show message placeholder
            // Update message based on whether a segmenter is loaded
             if (segmenter) {
                loadingMessage.innerText = `Camera stopped. Click "Start Camera" to restart or change config.`;
             } else {
                 loadingMessage.innerText = `Camera stopped. No model loaded. Select a model type.`;
             }

            // Re-enable config dropdown and file input when camera stops
            modelTypeSelect.disabled = false;
            backgroundImageInput.disabled = false;
            fileInputButtons.forEach(btn => btn.disabled = false);
            if (backgroundImageInput.value) {
                clearBackgroundButton.disabled = false;
             }

            // Disable fabric controls when camera stops
            addTextButton.disabled = true;
            deleteSelectedButton.disabled = true;
            fabricControlsDiv.style.pointerEvents = 'none'; // Disable clicks on container
        }

        function disposeFabricCanvas() {
             if (fabricCanvas) {
                 // Remove all event listeners added manually
                  fabricCanvas.off('selection:created', updateDeleteButtonState);
                  fabricCanvas.off('selection:updated', updateDeleteButtonState);
                  fabricCanvas.off('selection:cleared', updateDeleteButtonState);
                  fabricCanvas.off('object:added', updateDeleteButtonState);
                  fabricCanvas.off('object:removed', updateDeleteButtonState);

                 fabricCanvas.dispose(); // Clean up Fabric.js instance
                 fabricCanvas = null;
                 console.log("Fabric.js canvas disposed.");
             }
             videoObject = null; // Ensure this is also cleared
        }


        // --- Video Processing Loop ---
        async function processFrame() {
            // Check video readiness (readyState >= 2 means HAVE_CURRENT_DATA or more)
            // Also check if segmenter is still valid and processing is enabled
            if (!isProcessing || !segmenter || !videoInput.srcObject || videoInput.readyState < 2 || typeof segmenter.segmentPeople !== 'function' || !fabricCanvas) {
                 // Stop loop if processing is no longer needed, resources aren't ready,
                 // video isn't playing, segmenter became invalid, or fabricCanvas isn't initialized
                 rafId = null;
                 console.warn("Stopping processFrame loop due to invalid state.");
                 return;
            }

            // Ensure temporary canvas exists and matches output canvas size (Fabric.js canvas size)
            // Note: Fabric.js manages videoOutput dimensions, so we check fabricCanvas size.
            if (!tempCanvas || tempCanvas.width !== fabricCanvas.width || tempCanvas.height !== fabricCanvas.height) {
                 tempCanvas.width = fabricCanvas.width;
                 tempCanvas.height = fabricCanvas.height;
                 tempCtx = tempCanvas.getContext('2d', { willReadFrequently: true });
            } else if (!tempCtx) {
                tempCtx = tempCanvas.getContext('2d', { willReadFrequently: true }); // Ensure context is available
            }


            try {
                // 1. Draw the current video frame onto the temporary canvas
                tempCtx.clearRect(0, 0, tempCanvas.width, tempCanvas.height);
                tempCtx.drawImage(videoInput, 0, 0, tempCanvas.width, tempCanvas.height);

                // 2. Perform segmentation
                const segmentationEstimations = await segmenter.segmentPeople(videoInput);

                // 3. Apply the mask to the temporary canvas if segmentation data exists
                if (segmentationEstimations && segmentationEstimations.length > 0) {
                    const segmentation = segmentationEstimations[0]; // MediaPipe returns one segmentation

                    if (segmentation && segmentation.mask && typeof segmentation.mask.toCanvasImageSource === 'function') {
                        const mask = await segmentation.mask.toCanvasImageSource();

                        // Use 'destination-in' on the temporary canvas to cut out the person
                        tempCtx.globalCompositeOperation = 'destination-in';
                        tempCtx.drawImage(mask, 0, 0, tempCanvas.width, tempCanvas.height);
                        tempCtx.globalCompositeOperation = 'source-over'; // Reset composite operation
                    } else {
                         // console.warn("Segmentation mask or its method not available for this frame. Showing full video frame.");
                         // If mask is missing, the temp canvas already has the full video frame (step 1).
                         // Do nothing here, the full frame will be used for the videoObject.
                    }
                } else {
                    // If no segmentation found (e.g., no person detected)
                    // console.warn("No segmentation estimations found. Showing full video frame.");
                    // The temp canvas already has the full video frame (step 1).
                    // Do nothing here, the full frame will be used for the videoObject.
                }

                // 4. Convert the temporary canvas (containing the masked or full video frame) to an image source (Data URL is simple)
                const videoFrameDataUrl = tempCanvas.toDataURL();

                // 5. Update or create the Fabric.js image object for the video feed
                if (!videoObject) {
                    // Create the video object the first time
                    videoObject = new fabric.Image(videoFrameDataUrl, {
                        left: 0, top: 0,
                        selectable: false, // Cannot select the video feed itself
                        evented: false, // Does not participate in mouse events
                        lockMovementX: true, // Cannot be moved
                        lockMovementY: true,
                        lockRotation: true, // Cannot be rotated
                        lockScalingX: true, // Cannot be scaled
                        lockScalingY: true,
                        hasControls: false, // No controls to show
                        hasBorders: false, // No borders to show
                        objectCaching: false // Disable caching as the image changes every frame
                        // Add crossOrigin: 'anonymous' if loading video from external URL, not needed for dataUrl or local stream
                    });
                    // Add it to the canvas and send it to the back (index 0)
                    fabricCanvas.add(videoObject);
                    fabricCanvas.sendToBack(videoObject); // Ensure video is always at the bottom
                    console.log("Fabric video object created and added.");

                } else {
                    // Update the source of the existing video object
                    // The callback is optional here if you just need to update the image data
                    // and rely on renderAll() later. If setSrc also handles async loading,
                    // a callback might be needed to ensure render happens AFTER load.
                    // For data URLs, it's usually synchronous enough, or renderAll() handles it.
                    videoObject.setSrc(videoFrameDataUrl, null, { crossOrigin: 'anonymous' }); // Pass null for callback if not needed
                     // Note: setSrc with callback structure: videoObject.setSrc(url, function() { fabricCanvas.renderAll(); });
                     // If using the callback method, the requestAnimationFrame(processFrame) would go INSIDE that callback.
                     // For simplicity and consistent frame rate, we'll call renderAll and raf outside the setSrc callback.
                }


                // 6. Tell Fabric.js to render the canvas
                // This draws the background image, the videoObject, and all other Fabric objects on top.
                fabricCanvas.renderAll();


            } catch (error) {
                 console.error("Error processing frame:", error);
                 // If an error occurs during segmentation or drawing, stop the loop
                 isProcessing = false;
                 rafId = null;
                 loadingMessage.style.display = 'block';
                 loadingMessage.innerText = `Error processing video: ${error.message}`;
                 stopCamera(); // Attempt to stop camera cleanly
                 return; // Exit the function early
            }

            // Request the next frame only if still processing
            if (isProcessing) {
               // Request the next frame. This happens regardless of setSrc callback
               // which maintains a consistent requestAnimationFrame loop speed.
               rafId = requestAnimationFrame(processFrame);
            }
        }

        // --- Recording ---
        function startRecording() {
            recordedChunks = []; // Clear previous chunks
            // Capture stream from the canvas element (the processed video + fabric objects)
            // Use the current dimensions of the output canvas (managed by Fabric)
            const canvasStream = videoOutput.captureStream ? videoOutput.captureStream(30) : null; // 30fps capture

            if (!canvasStream) {
                 console.error("canvas.captureStream is not supported in this browser.");
                 alert("Video recording is not supported in your browser.");
                 return;
            }

            // Combine with audio stream if camera access included audio
             let streamToRecord = canvasStream;
            if (stream && stream.getAudioTracks().length > 0) {
                 const audioTracks = stream.getAudioTracks();
                 // Create a new MediaStream just for the audio tracks (optional, but clean)
                 const audioStream = new MediaStream(audioTracks);
                // Add audio tracks from the audio stream to the canvas stream
                audioTracks.forEach(track => streamToRecord.addTrack(track));
            }


            // Check if MediaRecorder is available
            if (typeof MediaRecorder === 'undefined') {
                 console.error("MediaRecorder is not supported in this browser.");
                 alert("Video recording is not supported in your browser.");
                 return;
            }

            // Check for supported MIME types and codecs
             const mimeTypes = [
                'video/webm;codecs=vp9,opus', // VP9 video with Opus audio
                'video/webm;codecs=vp8,opus', // VP8 video with Opus audio (more compatible)
                'video/webm', // General webm (codec decided by browser)
                'video/mp4;codecs=h264,aac', // H.264 video with AAC audio (less compatible on some browsers/OSs)
                // Add other potential types like 'video/webm;codecs=h264' or 'video/mp4;codecs=avc1' if needed
            ];

            let supportedType = null;
            for (const type of mimeTypes) {
                if (MediaRecorder.isTypeSupported(type)) {
                    supportedType = type;
                    break;
                }
            }

            if (!supportedType) {
                console.error("No supported video MIME type found for MediaRecorder.");
                alert("Your browser does not support recording in a compatible format.");
                return;
            }
            console.log(`Using MIME type for recording: ${supportedType}`);


            mediaRecorder = new MediaRecorder(streamToRecord, { mimeType: supportedType });

            mediaRecorder.ondataavailable = event => {
                if (event.data.size > 0) {
                    recordedChunks.push(event.data);
                }
            };

            mediaRecorder.onstop = () => {
                const blob = new Blob(recordedChunks, { type: supportedType }); // Use detected supported type
                const url = URL.createObjectURL(blob);

                downloadLink.href = url;
                // Suggest filename based on type
                let fileExtension = 'webm'; // Default to webm
                if (supportedType.includes('mp4')) {
                    fileExtension = 'mp4';
                }
                 // You could add more checks here for other types if needed

                downloadLink.download = `background-removed-video.${fileExtension}`;
                downloadLink.innerText = 'Download Recording';
                downloadLink.style.display = 'block';

                // Revoke URL after a delay or when clicked to free up memory
                downloadLink.onclick = () => {
                     setTimeout(() => { URL.revokeObjectURL(url); }, 100);
                     // Ensure the link is clickable after showing it
                     downloadLink.style.pointerEvents = 'auto';
                 }
                 // Ensure the download link is initially clickable when shown
                 downloadLink.style.pointerEvents = 'auto';
            };

             mediaRecorder.onerror = (event) => {
                console.error("MediaRecorder error:", event.error);
                alert("Error during recording: " + event.error.message);
                startRecordingButton.disabled = false;
                stopRecordingButton.disabled = true;
                // Hide download link on error
                downloadLink.style.display = 'none';
                downloadLink.href = '';
                downloadLink.style.pointerEvents = 'none'; // Make link unclickable
            };


            try {
                mediaRecorder.start(); // Start recording
                console.log("Recording started");
                startRecordingButton.disabled = true;
                stopRecordingButton.disabled = false;
                downloadLink.style.display = 'none'; // Hide download link while recording
                downloadLink.href = '';
                downloadLink.style.pointerEvents = 'none'; // Make link unclickable while recording
            } catch (error) {
                 console.error("Error starting MediaRecorder:", error);
                 alert("Could not start recording: " + error.message);
                 startRecordingButton.disabled = false;
                 stopRecordingButton.disabled = true;
                 downloadLink.style.display = 'none';
                 downloadLink.href = '';
                 downloadLink.style.pointerEvents = 'none';
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                 console.log("Recording stopped");
            }
            startRecordingButton.disabled = false;
            stopRecordingButton.disabled = true;
        }

        // --- Fabric.js Object Management ---

        // Load an image and add it to the canvas
        function addImageToCanvas(file) {
            if (!fabricCanvas) {
                console.warn("Fabric canvas not initialized.");
                return;
            }
            const reader = new FileReader();
            reader.onload = (e) => {
                fabric.Image.fromURL(e.target.result, function(img) {
                    // Set initial properties (optional)
                    img.set({
                        left: 50,
                        top: 50,
                        // Scale down large images if needed
                        scaleX: Math.min(1, (fabricCanvas.width / 4) / img.width),
                        scaleY: Math.min(1, (fabricCanvas.height / 4) / img.height),
                        // Ensure it doesn't go behind the video object
                        // zIndex is not a direct property, order is determined by `add` or `sendToBack/BringToFront`
                    });
                    fabricCanvas.add(img); // Add the image to the canvas
                    fabricCanvas.renderAll(); // Redraw the canvas to show the new object
                    console.log("Image added to canvas.");
                }, { crossOrigin: 'anonymous' }); // Use anonymous for cross-origin images if any
            };
             reader.onerror = (error) => {
                 console.error("Error reading image file for adding:", error);
                 alert("Error reading image file.");
             }
            reader.readAsDataURL(file); // Read the file as a data URL
        }

        // Add a text object to the canvas
        function addTextToCanvas() {
             if (!fabricCanvas) {
                console.warn("Fabric canvas not initialized.");
                return;
            }
            const defaultText = 'Your Text Here';
            const text = new fabric.IText(defaultText, {
                left: 100,
                top: 100,
                fill: '#000', // Default text color black
                fontFamily: 'sans-serif',
                fontSize: 24,
                // Add other properties like fontWeight, fontStyle, underline, etc.
            });

            fabricCanvas.add(text); // Add the text to the canvas
            fabricCanvas.setActiveObject(text); // Select the text object so it's ready for editing
            fabricCanvas.renderAll(); // Redraw the canvas
            console.log("Text added to canvas.");
        }

        // Delete selected objects
        function deleteSelectedObjects() {
            if (!fabricCanvas) {
                console.warn("Fabric canvas not initialized.");
                return;
            }
            const activeObjects = fabricCanvas.getActiveObjects(); // Get all selected objects
            if (activeObjects.length > 0) {
                activeObjects.forEach(obj => {
                    fabricCanvas.remove(obj); // Remove each selected object
                });
                fabricCanvas.discardActiveObject(); // Deselect the objects
                fabricCanvas.renderAll(); // Redraw the canvas
                console.log(`${activeObjects.length} object(s) deleted.`);
            } else {
                console.log("No object selected to delete.");
            }
            updateDeleteButtonState(); // Update button state after potential deletion
        }

        // Helper to update the delete button's disabled state
        function updateDeleteButtonState() {
             if (!fabricCanvas) {
                 deleteSelectedButton.disabled = true;
                 return;
             }
             const activeObjects = fabricCanvas.getActiveObjects();
             // Enable the delete button only if there is at least one selected object
             deleteSelectedButton.disabled = activeObjects.length === 0;
        }


        // --- Background Image Handling ---
        backgroundImageInput.addEventListener('change', (event) => {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = (e) => {
                    // Use Fabric.js to load the image and set it as canvas background
                    if (fabricCanvas) {
                         fabric.Image.fromURL(e.target.result, function(img) {
                             // Set background image to cover the canvas
                             // Fabric has a scaling option for background images
                             fabricCanvas.setBackgroundImage(img, fabricCanvas.renderAll.bind(fabricCanvas), {
                                 // Scale the image to cover the canvas while maintaining aspect ratio
                                 scaleX: fabricCanvas.width / img.width,
                                 scaleY: fabricCanvas.height / img.height,
                                 // Using `cover` option (if available in newer Fabric versions) is simpler
                                 // if not, calculate scale and centering manually like before, but for Fabric's setBackgroundImage options
                                 // Eg: { scaleX: ratio, scaleY: ratio, originX: 'center', originY: 'center', top: fabricCanvas.height / 2, left: fabricCanvas.width / 2 }
                                 // For simplicity using just scale here which might stretch or tile if img < canvas
                                 // Let's use the manual calculation from before for better 'cover' effect
                                 originX: 'left',
                                 originY: 'top',
                                 // Calculate scale to cover
                                 scaleX: Math.max(fabricCanvas.width / img.width, fabricCanvas.height / img.height),
                                 scaleY: Math.max(fabricCanvas.width / img.width, fabricCanvas.height / img.height),
                                 // Calculate centering offsets
                                 left: (fabricCanvas.width - img.width * Math.max(fabricCanvas.width / img.width, fabricCanvas.height / img.height)) / 2,
                                 top: (fabricCanvas.height - img.height * Math.max(fabricCanvas.width / img.width, fabricCanvas.height / img.height)) / 2
                             });
                             console.log("Background image set via Fabric.js");
                             // Redraw is handled by the callback
                         }, { crossOrigin: 'anonymous' }); // Use anonymous if loading from external URL
                    } else {
                        console.warn("Fabric canvas not initialized. Cannot set background image.");
                        alert("Cannot set background image: Camera is not started or model not loaded.");
                        clearBackgroundImage(); // Clear input state if canvas isn't ready
                        return; // Exit early if fabricCanvas is null
                    }

                    backgroundFileNameSpan.innerText = file.name; // Display filename
                    clearBackgroundButton.style.display = 'inline-block'; // Show clear button
                    clearBackgroundButton.disabled = false; // Enable clear button
                    console.log("Background image loaded:", file.name);

                };
                reader.onerror = (error) => {
                     console.error("Error reading file:", error);
                     alert("Error reading image file.");
                      clearBackgroundImage(); // Clear potentially partial state
                 }
                reader.readAsDataURL(file); // Read the file as a data URL
            } else {
                // If file input is cleared or cancelled
                clearBackgroundImage();
            }
        });

        clearBackgroundButton.addEventListener('click', () => {
            clearBackgroundImage();
        });

        function clearBackgroundImage() {
             if (fabricCanvas) {
                 fabricCanvas.setBackgroundImage(null, fabricCanvas.renderAll.bind(fabricCanvas)); // Remove the background image
                 console.log("Background image removed via Fabric.js");
             } else {
                 console.warn("Fabric canvas not initialized. Cannot clear background image.");
             }

             backgroundImageInput.value = null; // Clear the file input itself
             backgroundFileNameSpan.innerText = 'No image selected'; // Reset filename display
             clearBackgroundButton.style.display = 'none'; // Hide clear button
             clearBackgroundButton.disabled = true; // Disable clear button
             console.log("Background image cleared.");
        }


        // --- Event Listeners ---
        startButton.addEventListener('click', startCamera);
        stopButton.addEventListener('click', stopCamera);
        startRecordingButton.addEventListener('click', startRecording);
        stopRecordingButton.addEventListener('click', stopRecording);

        // Add event listener for the model type dropdown
        modelTypeSelect.addEventListener('change', loadModel);

        // Add event listeners for new Fabric controls
        addImageInput.addEventListener('change', (event) => {
             const file = event.target.files[0];
             if (file) {
                 addImageToCanvas(file);
                 addImageInput.value = null; // Clear input so same file can be added again
             }
        });
        addTextButton.addEventListener('click', addTextToCanvas);
        deleteSelectedButton.addEventListener('click', deleteSelectedObjects);


        // Stop camera and processing when the page is closed/navigated away
        window.addEventListener('beforeunload', () => {
            stopCamera(); // Ensure tracks are stopped and processing loop ends
             if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop(); // Stop recording if ongoing
            }
            // Attempt to dispose of the model and Fabric canvas
             if (segmenter && typeof segmenter.dispose === 'function') {
                segmenter.dispose().catch(e => console.error("Error disposing segmenter on unload:", e));
             }
             disposeFabricCanvas(); // Dispose Fabric.js instance
        });

        // --- Initialization ---
        // Wait for the window to fully load (including external scripts) before loading the model
        window.onload = () => {
            loadModel(); // Initial model load based on default dropdown selection
            // Ensure the clear button is initially hidden
            clearBackgroundButton.style.display = 'none';
            clearBackgroundButton.disabled = true;
            // Ensure Fabric controls are initially disabled
            addTextButton.disabled = true;
            deleteSelectedButton.disabled = true;
            fabricControlsDiv.style.pointerEvents = 'none';
        };
    </script>

</body>
</html>